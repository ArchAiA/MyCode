{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the API_KEY and API_SECRET with your application's key and secret.\n",
    "consumer_key = 'XxMbTu6bFIU7v4agH3OqRDDYi'\n",
    "consumer_secret = 'wpF7eKmVDlH3zJzvORWD0Pr6j8dqAhczrIFq4c3ZAH0g3kDw5Q'\n",
    "access_token = '722589135987478528-SxCJ6LxQaM6A1pa3ZTUj5CYWVZkW48m'\n",
    "access_secret = 'ChZ6oU0B6ALmrI1bE9GaW9nXD7c8w3nSdtGArwnHAe6Ho'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to collect tweets using Python's Tweepy library.\n",
    "\n",
    "<!--more-->\n",
    "\n",
    "TABLE OF CONTENTS\n",
    "\n",
    "This is the third post in an on-going Pokemon Go analysis series.  Last time, we discussed Twitter's Search and Streaming APIs.  In this post, we'll discuss how to use Python's Tweepy library to interface with Twitter's API and start collecting tweets about Pokemon Go teams.\n",
    "\n",
    "\n",
    "We'll cover the following topics:\n",
    "\n",
    "1. [The Tweepy Library](#tweepy)\n",
    "2. [Using the Search API to collect historical tweets](#search)\n",
    "3. [Using the Streaming API to collect new tweets](#stream)\n",
    "\n",
    "# <a name=\"tweepy\"></a> The Tweepy Library\n",
    "\n",
    "There are a number of Python libraries that connect to the twitter API.  Twitter provides a [full list of API libraries](https://dev.twitter.com/overview/api/twitter-libraries) for both Python and other languages. Some of the most popular among these include [python-twitter](https://github.com/bear/python-twitter), [Twython](https://twython.readthedocs.io/en/latest/), and [Tweepy](http://www.tweepy.org/).  I'm most familiar with Tweepy, so we'll be using that for our analysis.  In practice, Tweepy is a simple wrapper for the Twitter API.  It simplifies the interaction between Python and Twitter's API by handling complications such as rate limiting behind the scenes.  Installing Tweepy is as simple as typing `pip install tweepy` into a command line.  For more information about Tweepy, read the [developer's introduction to Tweepy](http://tweepy.readthedocs.io/en/v3.5.0/getting_started.html#introduction).  \n",
    "\n",
    "\n",
    "# <a name=\"search\"></a> Using the Search API to collect historical tweets\n",
    "\n",
    "In this section, we'll look at how to collect historical tweets using Tweepy and Twitter's Search API.  The first thing we need to do is import the `sys`, `os`, `jsonpickle`, and `tweepy` libraries into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import sys\n",
    "import os\n",
    "import jsonpickle\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our goal is to make a map of the United States which displays the dominance of each team in each state.  Since we aren't interested in tweets from outside of the United States, we'll want to add a location filter when we use the Search API.  This requires us to use the [Geo Search REST API](https://dev.twitter.com/rest/reference/get/geo/search) to find Twitter's `place_id` for the United States.  Note from the [geo_search documentation](https://dev.twitter.com/rest/reference/get/geo/search) that we have to use user-authentication for such a query.  We set up user authentication in Tweepy with the following commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pass our consumer key and consumer secret to Tweepy's user authentication handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "#Pass our access token and access secret to Tweepy's user authentication handler\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "#Creating a twitter API wrapper using tweepy\n",
    "#Details here http://docs.tweepy.org/en/v3.5.0/api.html\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Error handling\n",
    "if (not api):\n",
    "    print (\"Problem connecting to API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to replace `consumer_key`,`consumer_secret`,`access_token`, and `access_secret` with your personal keys and secret.  Once we have the API object instantiated we can use Tweepy's geo_search method to query the Geo Search API: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA id is:  96683cc9126741d1\n"
     ]
    }
   ],
   "source": [
    "#Getting Geo ID for USA\n",
    "places = api.geo_search(query=\"USA\", granularity=\"country\")\n",
    "\n",
    "#Copy USA id\n",
    "place_id = places[0].id\n",
    "print('USA id is: ',place_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we know which `place_id` to use as a filter in the Search API.  Next, we'll use the Search API to collect historical tweets. Recall: \n",
    "\n",
    "* The search API only collects a random sample of tweets \n",
    "* There is no way to get more than the last week of tweets without purchasing them from a third-party provider\n",
    "\n",
    "From the Search API documentation, note that we can use either user-authentication or application-only authentication to access the Search API.  User-authentication allows for 180 queries per access token every 15 minutes, and application-only authentication allows for 450 queries total.  Since we only have one access token to work with, we'll want to switch to application-only authentication for the higher rate limit.  Tweepy has a separate `AppAuthHandler` method for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Switching to application authentication\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "#Setting up new api wrapper, using authentication only\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    " \n",
    "#Error handling\n",
    "if (not api):\n",
    "    print (\"Problem Connecting to API\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that we added two arguements to the API wrapper this time around.  The Search API is returns a maximum of 100 tweets per query.  We have 450 queries available to us every 15 minutes using application authentication, so we can collect 45,000 tweets every 15 minutes.  If we exceed this rate, the `wait_on_rate_limit` arguement tells tweepy to pause until we are no longer rate limited. If `wait_on_rate_limit` is set to the default value of `False`, Tweepy would just return an error and stop search once we reached out rate limit.  The `wait_on_rate_limit_notify` arguement tells Tweepy to display a notification when we've reached the rate limit and that it is waiting for the 15 minute rate limit window to refresh.  These arguements were also available when we were using user-authentication with the Geo Search API, but we did not need to use them since we were only sending one query.  As a side note, if you ever want to check the status of your rate limits you can use Tweepy's `rate_limit_status` method, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/search/tweets': {'limit': 450, 'remaining': 450, 'reset': 1470850812}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can check how many queries you have left using rate_limit_status() method\n",
    "api.rate_limit_status()['resources']['search']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an API object using application-only authentication, we're ready to query the Search API for tweets about each Pokemon Go team.  First, we define the search query that we'll send to the API.  In this case, we'll want to search for any phrase that is likely to identify tweets about each Pokemon Go team, but will not grab generic tweets about Pokemon, Pokemon Go, or other topics.  After looking through trending hashtags and tweets about Pokemon Go, I chose the following list of phrases:\n",
    "\n",
    "* #teammystic\n",
    "* #teaminstinct\n",
    "* #teamvalor\n",
    "* #teamblue\n",
    "* #teamyellow\n",
    "* #teamred\n",
    "* #mystic\n",
    "* #valor\n",
    "* #instinct\n",
    "* \"team mystic\"\n",
    "* \"team valor\"\n",
    "* \"team instinct\"\n",
    "\n",
    "\n",
    "Note that the search phrases are not case sensitive.  We can search for all of these terms at once by seperating them with `OR` in our query.  I've also included a filter that requires the tweet originate within the United Statesby including `place:96683cc9126741d1` at the beginning of the query, where `96683cc9126741d1` is the US `place_id` we found with the Geo Search API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This is what we are searching for\n",
    "#We can restrict the location of tweets using place:id \n",
    "#We can search for multiple phrases using OR\n",
    "searchQuery = 'place:96683cc9126741d1 #teammystic OR #teaminstinct OR #teamvalor OR' \\\n",
    "              '#teamblue OR #teamyellow OR #teamred OR' \\\n",
    "              '#mystic OR #valor OR #instinct OR' \\\n",
    "              '\"team mystic\" OR \"team valor\" OR \"team instinct\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define the maximum number of tweets that we want to collect, and the maximum number of tweets that we want to recieve from each query that we send:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Maximum number of tweets we want to collect \n",
    "maxTweets = 1000000\n",
    "\n",
    "#The twitter Search API allows up to 100 tweets per query\n",
    "tweetsPerQry = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Search API limits us to 100 tweets per query, and that we may not collect the number of tweets that we specified since the Search API only collects the past week of tweets.  The easieset way to send our query to the Search API is through [Tweepy's Cursor method](http://tweepy.readthedocs.io/en/v3.5.0/cursor_tutorial.html).  The Cursor will automatically send queries to the Search API until we have collected the maximum number of tweets that we specified, or until we reach the end of the Search API database.  We can iterate over each tweet that the Cursor gathers using a `for` loop, and write the JSON formatted tweet to a text file using the `jsonpickle` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2006 tweets\n"
     ]
    }
   ],
   "source": [
    "tweetCount = 0\n",
    "\n",
    "#Open a text file to save the tweets to\n",
    "with open('PoGo_USA_Tutorial.json', 'w') as f:\n",
    "\n",
    "    #Tell the Cursor method that we want to use the Search API (api.search)\n",
    "    #Also tell Cursor our query, and the maximum number of tweets to return\n",
    "    for tweet in tweepy.Cursor(api.search,q=searchQuery).items(maxTweets) :         \n",
    "\n",
    "        #Verify the tweet has place info before writing (It should, if it got past our place filter)\n",
    "        if tweet.place is not None:\n",
    "            \n",
    "            #Write the JSON format to the text file, and add one to the number of tweets we've collected\n",
    "            f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "            tweetCount += 1\n",
    "\n",
    "    #Display how many tweets we have collected\n",
    "    print(\"Downloaded {0} tweets\".format(tweetCount))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the information we are storing in the text file.  The Twitter API returns the tweet in [JSON format](http://json.org/), and Tweepy parses the JSON text into a number of class members. First, let's look at the raw JSON text that Twitter returns.  This is what we are saving to our text file in the `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributors': None,\n",
       " 'coordinates': None,\n",
       " 'created_at': 'Tue Aug 02 19:51:58 +0000 2016',\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'urls': [],\n",
       "  'user_mentions': [{'id': 873491544,\n",
       "    'id_str': '873491544',\n",
       "    'indices': [0, 13],\n",
       "    'name': 'Kenel M',\n",
       "    'screen_name': 'KxSweaters13'}]},\n",
       " 'favorite_count': 1,\n",
       " 'favorited': False,\n",
       " 'geo': None,\n",
       " 'id': 760563814450491392,\n",
       " 'id_str': '760563814450491392',\n",
       " 'in_reply_to_screen_name': 'KxSweaters13',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': 873491544,\n",
       " 'in_reply_to_user_id_str': '873491544',\n",
       " 'is_quote_status': False,\n",
       " 'lang': 'en',\n",
       " 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       " 'place': {'attributes': {},\n",
       "  'bounding_box': {'coordinates': [[[-71.813501, 42.4762],\n",
       "     [-71.702186, 42.4762],\n",
       "     [-71.702186, 42.573956],\n",
       "     [-71.813501, 42.573956]]],\n",
       "   'type': 'Polygon'},\n",
       "  'contained_within': [],\n",
       "  'country': 'United States',\n",
       "  'country_code': 'US',\n",
       "  'full_name': 'Leominster, MA',\n",
       "  'id': 'c4f1830ea4b8caaf',\n",
       "  'name': 'Leominster',\n",
       "  'place_type': 'city',\n",
       "  'url': 'https://api.twitter.com/1.1/geo/id/c4f1830ea4b8caaf.json'},\n",
       " 'retweet_count': 0,\n",
       " 'retweeted': False,\n",
       " 'source': '<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>',\n",
       " 'text': '@KxSweaters13 are you the kenelx13 I see owning leominster for team valor?',\n",
       " 'truncated': False,\n",
       " 'user': {'contributors_enabled': False,\n",
       "  'created_at': 'Thu Apr 21 17:09:52 +0000 2011',\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'description': \"Arbys when it's cold. Kimballs when it's warm. @Ally__09 all year. Comp sci classes sometimes.\",\n",
       "  'entities': {'description': {'urls': []}},\n",
       "  'favourites_count': 1106,\n",
       "  'follow_request_sent': None,\n",
       "  'followers_count': 167,\n",
       "  'following': None,\n",
       "  'friends_count': 171,\n",
       "  'geo_enabled': True,\n",
       "  'has_extended_profile': False,\n",
       "  'id': 285715182,\n",
       "  'id_str': '285715182',\n",
       "  'is_translation_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'lang': 'en',\n",
       "  'listed_count': 2,\n",
       "  'location': 'MA',\n",
       "  'name': 'Steve',\n",
       "  'notifications': None,\n",
       "  'profile_background_color': '131516',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme14/bg.gif',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme14/bg.gif',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/285715182/1462218226',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/727223698332200961/bGPjGjHK_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/727223698332200961/bGPjGjHK_normal.jpg',\n",
       "  'profile_link_color': '4A913C',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_sidebar_fill_color': 'EFEFEF',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'protected': False,\n",
       "  'screen_name': 'StephenBurke_',\n",
       "  'statuses_count': 5913,\n",
       "  'time_zone': 'Eastern Time (US & Canada)',\n",
       "  'url': None,\n",
       "  'utc_offset': -14400,\n",
       "  'verified': False}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's look at what we are actually printing to text  (if you don't know what JSON is: http://json.org/)\n",
    "tweet._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already mentioned, each one of the JSON attributes is turned into an individual class member in Tweepy's tweet object.  I've included a function below that will list all of these class members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_api\n",
      "_json\n",
      "author\n",
      "contributors\n",
      "coordinates\n",
      "created_at\n",
      "destroy\n",
      "entities\n",
      "favorite\n",
      "favorite_count\n",
      "favorited\n",
      "geo\n",
      "id\n",
      "id_str\n",
      "in_reply_to_screen_name\n",
      "in_reply_to_status_id\n",
      "in_reply_to_status_id_str\n",
      "in_reply_to_user_id\n",
      "in_reply_to_user_id_str\n",
      "is_quote_status\n",
      "lang\n",
      "metadata\n",
      "parse\n",
      "parse_list\n",
      "place\n",
      "retweet\n",
      "retweet_count\n",
      "retweeted\n",
      "retweets\n",
      "source\n",
      "source_url\n",
      "text\n",
      "truncated\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "#A function to clean up and print all of the JSON attributes \n",
    "def PrintMembers(obj):\n",
    "    for attribute in dir(obj):\n",
    "        \n",
    "        #We don't want to show built in methods of the class\n",
    "        if not attribute.startswith('__'):\n",
    "            print(attribute)\n",
    "            \n",
    "PrintMembers(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each additional layer within a JSON attribute will be parsed as an additional class member within the tweet object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_api\n",
      "attributes\n",
      "bounding_box\n",
      "contained_within\n",
      "country\n",
      "country_code\n",
      "full_name\n",
      "id\n",
      "name\n",
      "parse\n",
      "parse_list\n",
      "place_type\n",
      "url\n"
     ]
    }
   ],
   "source": [
    "#Can dig into each member as well\n",
    "PrintMembers(tweet.place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a closing note, I wanted to share the \"old\" way of using Tweepy.  Before the Cursor method was introduced, the process of sending multiple queries until you collected your maximum number of tweets all had to be handled manually.  You will often see old implementations online which do so, but when possible you should use the Cursor instead.  I've included an example of our search query using the \"old\" way below.  I hope the example will help you recognize and update any out of date implementations you may come across in your own work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Old way of doing things - which is what you will see a lot of people doing online\n",
    "max_id = -1\n",
    "tweetCount = 0\n",
    "with open('PoGo_USA_Tutorial.json', 'w') as f:\n",
    "    #While we still want to collect more tweets\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            #Look for more tweets, resuming where we left off\n",
    "            if max_id <= 0:\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "            else:\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id - 1))\n",
    "            \n",
    "            #If we didn't find any exit the loop\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            \n",
    "            #Write the JSON output of any new tweets we found to the output file\n",
    "            for tweet in new_tweets:\n",
    "                \n",
    "                #Make sure the tweet has place info before writing\n",
    "                if (tweet.place is not None) and (tweetCount < maxTweets):\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                        '\\n')\n",
    "                    tweetCount += 1\n",
    "                    \n",
    "            #Display how many tweets we have collected\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            \n",
    "            #Record the id of the last tweet we looked at\n",
    "            max_id = new_tweets[-1].id\n",
    "            \n",
    "        except tweepy.TweepError as e:\n",
    "            \n",
    "            #Print the error and continue searching\n",
    "            print(\"some error : \" + str(e))\n",
    "\n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"stream\"></a> Using the Streaming API to collect new tweets\n",
    "\n",
    "If we want to collect new tweets in real-time, we'll need to use Twitter's Streaming API.  Since we don't need to worry about rate limits, we'll switch back to using Tweepy's user-authentication handler for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Connecting to the twitter streaming API \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Tweepy documentation page](http://docs.tweepy.org/en/v3.4.0/streaming_how_to.html) tells us how to set up a Streaming API interface.  We have to follow three steps:\n",
    "\n",
    "1. Create a class inheriting from StreamListener\n",
    "2. Use that class create a Stream object\n",
    "3. Connect to the Twitter API using the Stream.\n",
    "\n",
    "First, we'll discuss how to inherit and modify the `StreamListener` class.  The are two ways to interact with the streaming data - by overloading the `on_data` method or by overloading the `on_status` method.  If you want detailed information about the tweets you need to override the `on_data` method. This will allow us to interact with the data at the highest level and opens up information such as replies to statuses, deletions, direct messages, friends, etc.  The default `on_data` method from the `StreamListener` class passes data from statuses to the `on_status` method. If we're only concerned with reading the JSON output of tweet statuses, can override the `on_status` method instead of the `on_data` method.  This process is shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Inherit from the StreamListener object\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    #Overload the on_status method\n",
    "    def on_status(self, status):\n",
    "        try:\n",
    "            \n",
    "            #Open a text file to save tweets to\n",
    "            with open('PoGo.json', 'a') as f:\n",
    "                \n",
    "                #Check if the tweet has coordinates, if so write it to text\n",
    "                if (status.coordinates is not None):\n",
    "                    f.write(status)\n",
    "                return True\n",
    "         \n",
    "        #Error handling\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_status: %s\" % str(e))\n",
    "            \n",
    "        return True\n",
    " \n",
    "    #Error handling\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "    #Timeout handling\n",
    "    def on_timeout(self):\n",
    "        return True \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `MyStreamListener` class, we can use it to create a class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a stream object  \n",
    "twitter_stream = tweepy.Stream(auth, MyStreamListener())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Tweepy documentation](http://docs.tweepy.org/en/v3.4.0/streaming_how_to.html) discusess a number of ways to start the Twitter stream.  In our case, we'll want to use the `filter` method to track particular phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_stream.filter(track=['#teammystic','teaminstinct','#teamvalor', \\\n",
    "                             '#teamblue','#teamyellow','#teamred',\\\n",
    "                             '#mystic','#instinct','#valor', \\\n",
    "                             'team mystic','team instinct','team valor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, we could filter on [other parameters](https://dev.twitter.com/streaming/overview/request-parameters), such as locations:\n",
    "\n",
    "`twitter_stream.filter(locations=[-122.75,36.8,-121.75,37.8])`\n",
    "\n",
    "It is important to note that some filters don't interact as expected with each other.  For instance, look at the information about the location parameter in the previous link.  It states that \"Bounding boxes do not act as filters for other filter parameters. For example track=twitter&locations=-122.75,36.8,-121.75,37.8 would match any tweets containing the term Twitter (even non-geo tweets) OR coming from the San Francisco area.\"  Therefore, if we tried to filter on both location and phrases at the same time, we would not get tweets that contain the phrase AND that are from the location.  Instead, we'd get tweets that contain the phrase OR are from the location.  This is why we need the \n",
    "\n",
    "`if (status.coordinates is not None):`\n",
    "\n",
    "condition in our class definition.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
