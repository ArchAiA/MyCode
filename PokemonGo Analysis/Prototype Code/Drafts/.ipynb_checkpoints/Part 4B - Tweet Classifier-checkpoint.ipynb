{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following: http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\n",
    "Could do something like https://marcobonzanini.com/2015/05/17/mining-twitter-data-with-python-part-6-sentiment-analysis-basics/ but unsupervised sentiment analysis typically doesn't work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "#need to run nltk.download() from command line and get stopwords corupus (freezes in the notebook)\n",
    "\n",
    "import pandas\n",
    "import pickle\n",
    "import os.path\n",
    "import string\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import precision as prec\n",
    "from nltk.metrics import recall as rec\n",
    "from nltk.metrics import f_measure as fmeas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the csv dataframe\n",
    "PoGo_labeled = pandas.read_csv('PoGo_Sentiment_Labeled_extended.csv')\n",
    "PoGo_labeled.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latt</th>\n",
       "      <th>location</th>\n",
       "      <th>long</th>\n",
       "      <th>multi-team</th>\n",
       "      <th>screenName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Caldwell, ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>desmond_ayala</td>\n",
       "      <td>pos</td>\n",
       "      <td>Which pokemon go team did y'all chose? #valor</td>\n",
       "      <td>2.953472e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.918741</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>40.694338</td>\n",
       "      <td>False</td>\n",
       "      <td>aphrospice</td>\n",
       "      <td>pos</td>\n",
       "      <td>#Magikarp practicing his struggle skills in th...</td>\n",
       "      <td>1.629086e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bixby, OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>ABellgowan</td>\n",
       "      <td>pos</td>\n",
       "      <td>Pokemon Go is taking over my life #TeamInstinct</td>\n",
       "      <td>1.681036e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>JangoSnow</td>\n",
       "      <td>pos</td>\n",
       "      <td>Go Team Instinct! I like underdogs. :)  https:...</td>\n",
       "      <td>1.057434e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Niagara Falls, NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>EmberLighta2</td>\n",
       "      <td>pos</td>\n",
       "      <td>#TeamMystic has total control of Niagara Falls!!</td>\n",
       "      <td>7.513920e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        latt           location       long multi-team     screenName  \\\n",
       "0        NaN       Caldwell, ID        NaN      False  desmond_ayala   \n",
       "1 -73.918741       Brooklyn, NY  40.694338      False     aphrospice   \n",
       "2        NaN          Bixby, OK        NaN      False     ABellgowan   \n",
       "3        NaN    Los Angeles, CA        NaN      False      JangoSnow   \n",
       "4        NaN  Niagara Falls, NY        NaN      False   EmberLighta2   \n",
       "\n",
       "  sentiment                                               text        userId  \n",
       "0       pos      Which pokemon go team did y'all chose? #valor  2.953472e+09  \n",
       "1       pos  #Magikarp practicing his struggle skills in th...  1.629086e+07  \n",
       "2       pos    Pokemon Go is taking over my life #TeamInstinct  1.681036e+09  \n",
       "3       pos  Go Team Instinct! I like underdogs. :)  https:...  1.057434e+07  \n",
       "4       pos   #TeamMystic has total control of Niagara Falls!!  7.513920e+17  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PoGo_labeled.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Making lists of positive and negative tweets\n",
    "pos_tweets = [(PoGo_labeled.ix[row,'text'],'positive') for row in range(len(PoGo_labeled)) if \\\n",
    "              PoGo_labeled.ix[row,'sentiment'] == 'pos']\n",
    "\n",
    "neg_tweets = [(PoGo_labeled.ix[row,'text'],'negative') for row in range(len(PoGo_labeled)) if \\\n",
    "              PoGo_labeled.ix[row,'sentiment'] == 'neg']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Could do the following to cut out common words: (found it doesn't help in this case)<br \\>\n",
    "from nltk.corpus import stopwords <br \\>\n",
    "stopset = set(stopwords.words('english')) <br \\>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use top feature list in classifier? (yes/no) : yes\n"
     ]
    }
   ],
   "source": [
    "#If we've made one, import top features list\n",
    "use_top_feature = False\n",
    "\n",
    "if os.path.isfile('top_features.txt'):\n",
    "    with open('top_features.txt', 'rb') as f:\n",
    "        use_top_feature_q=input('Use top feature list in classifier? (yes/no) : ')\n",
    "        if use_top_feature_q == 'yes':\n",
    "            use_top_feature=True\n",
    "        top_feature_list = pickle.load(f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Exclude words that can identify the team from list of features\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "excluded_words = ['teammystic','mystic','teamblue','blue',\\\n",
    "                  'teaminstinct','instinct','teamyellow','yellow',\\\n",
    "                  'teamvalor','valor','teamred','red']\n",
    "\n",
    "#filter the tweets to produce a list of features\n",
    "def filter_tweets(tweets):\n",
    "    filtered_tweets = []\n",
    "    \n",
    "    #Get a list of words, and the sentiment for each tweet\n",
    "    for (words, sentiment) in tweets: \n",
    "        words_filtered=[]\n",
    "        \n",
    "        #For each word in the list of words, filter on some requirements.  If it passes, add it to features of that tweet\n",
    "        for word in words.split(): \n",
    "            \n",
    "            #Remove punctuation\n",
    "            word = ''.join(ch for ch in word if ch not in exclude)\n",
    "            \n",
    "            #Remove one letter words\n",
    "            if len(word) >= 1: \n",
    "                \n",
    "                    #treat URLs the same\n",
    "                    if word[:4] == 'http':\n",
    "                        word='http'\n",
    "                        \n",
    "                    #remove hashtags\n",
    "                    if word[0] == '#': \n",
    "                        word=word[1:]\n",
    "                        \n",
    "                    #remove team identifiers\n",
    "                    if (word.lower() not in excluded_words):\n",
    "        \n",
    "                        #require lower case\n",
    "                        words_filtered.append(word.lower()) \n",
    "\n",
    "        #Identify top 200 bigams in the filtered word list using chi_sq measure of importance\n",
    "        bigram_finder = BigramCollocationFinder.from_words(words_filtered)\n",
    "        bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 200)  \n",
    "        \n",
    "        #If we are using the top feature list, require that the unigram and bigrams be in the top_featue list\n",
    "        if use_top_feature:\n",
    "            filtered_tweets.append(([ngram for ngram in itertools.chain(words_filtered, bigrams) if str(ngram) in top_feature_list],sentiment))\n",
    "        else:\n",
    "            filtered_tweets.append(([ngram for ngram in itertools.chain(words_filtered, bigrams)],sentiment))\n",
    "\n",
    "    return filtered_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_pos_tweets = filter_tweets(pos_tweets)\n",
    "filtered_neg_tweets = filter_tweets(neg_tweets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Divide data into training, cross validation, and test sets\n",
    "\n",
    "#set mode to 'down' for downsampling\n",
    "#set mode to 'up', for upsampling\n",
    "#set mode to 'ratio', for an accurate ratio of pos/neg\n",
    "\n",
    "set_mode = 'down'\n",
    "\n",
    "total_neg = len(neg_tweets)\n",
    "total_pos = len(pos_tweets)\n",
    "\n",
    "if set_mode == 'down':\n",
    "    #Downsampling the positive tweets\n",
    "    \n",
    "    #half the negative tweets go in training\n",
    "    len_train = int(round(len(filtered_neg_tweets)/2)*2)\n",
    "    train_tweets = filtered_neg_tweets[:int(len_train/2)] + filtered_pos_tweets[:int(len_train/2)]\n",
    "\n",
    "    #half of the remaining half go in cv\n",
    "    cv_neg_cutoff = int( (len_train/2) + round((len(filtered_neg_tweets) - len_train/2)/2) )\n",
    "    cv_pos_cutoff = int( (len_train/2) + round((len(filtered_pos_tweets) - len_train/2)/2) )\n",
    "    cv_tweets =  filtered_neg_tweets[int(len_train/2):cv_neg_cutoff] +  filtered_pos_tweets[int(len_train/2):cv_pos_cutoff]  \n",
    "\n",
    "    #rest go into testing\n",
    "    test_tweets = filtered_neg_tweets[cv_neg_cutoff:] +  filtered_pos_tweets[cv_pos_cutoff:]  \n",
    "\n",
    "elif set_mode == 'up':\n",
    "    #Upsample negative tweets\n",
    "    \n",
    "    #half the negative tweets go in training\n",
    "    neg_scale_factor = 3\n",
    "    len_train = int(round(len(filtered_neg_tweets)/2)*2)\n",
    "    train_tweets = filtered_neg_tweets[:int(len_train/2)]*neg_scale_factor + filtered_pos_tweets[:int(neg_scale_factor*len_train/2)]\n",
    "\n",
    "    #half of the remaining half go in cv\n",
    "    cv_neg_cutoff = int( (len_train/2) + round((len(filtered_neg_tweets) - len_train/2)/2) )\n",
    "    cv_pos_cutoff = int( (neg_scale_factor*len_train/2) + round((len(filtered_pos_tweets) - neg_scale_factor*len_train/2)/2) )\n",
    "    cv_tweets =  filtered_neg_tweets[int(len_train/2):cv_neg_cutoff] +  filtered_pos_tweets[int(neg_scale_factor*len_train/2):cv_pos_cutoff]  \n",
    "\n",
    "    #rest go into testing\n",
    "    test_tweets =  filtered_neg_tweets[cv_neg_cutoff:] +  filtered_pos_tweets[cv_pos_cutoff:]  \n",
    "\n",
    "elif set_mode == 'ratio':\n",
    "    #True ratio of tweets\n",
    "    \n",
    "    #half the tweets go into training\n",
    "    len_neg_train = int(round(len(filtered_neg_tweets)*0.5))\n",
    "    len_pos_train = int(round(len(filtered_pos_tweets)*0.5))\n",
    "    train_tweets = filtered_neg_tweets[:int(len_neg_train/2)] + filtered_pos_tweets[:int(len_pos_train/2)]\n",
    "\n",
    "    #half of the remaining half go in cv\n",
    "    cv_neg_cutoff = int( (len_neg_train/2) + round((len(filtered_neg_tweets) - len_neg_train/2)/2) )\n",
    "    cv_pos_cutoff = int( (len_pos_train/2) + round((len(filtered_pos_tweets) - len_pos_train/2)/2) )\n",
    "    cv_tweets =  filtered_neg_tweets[int(len_neg_train/2):cv_neg_cutoff] +  filtered_pos_tweets[int(len_pos_train/2):cv_pos_cutoff]  \n",
    "\n",
    "    #rest go into testing\n",
    "    test_tweets =  filtered_neg_tweets[cv_neg_cutoff:] +  filtered_pos_tweets[cv_pos_cutoff:]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Making a list of all unigrams and bigrams in the tweets\n",
    "\n",
    "#Function to find all of the words in a tweet\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in tweets:\n",
    "      all_words.extend(words)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "#Function to make a list of features from a list of words\n",
    "def get_word_features(wordlist,min_freq):\n",
    "\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    sorted_word_list = sorted(wordlist.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_features = [sorted_word_list[word][0] for word in range(len(sorted_word_list)) if sorted_word_list[word][1] >= min_freq]\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(train_tweets),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature extractor - determines which word features are in each tweet\n",
    "def extract_features(document):\n",
    "\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % str(word)] = (word in document_words)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract training/test set from training/test tweets\n",
    "training_set = nltk.classify.apply_features(extract_features, train_tweets)\n",
    "cv_set = nltk.classify.apply_features(extract_features, cv_tweets)\n",
    "test_set = nltk.classify.apply_features(extract_features, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7083333333333334\n",
      "F-measure [negative]: 0.2048\n",
      "F-measure [positive]: 0.8214157384117858\n",
      "Precision [negative]: 0.11657559198542805\n",
      "Precision [positive]: 0.9896103896103896\n",
      "Recall [negative]: 0.8421052631578947\n",
      "Recall [positive]: 0.702088452088452\n",
      "Negative contamination improved by  77.51070589860477 percent\n"
     ]
    }
   ],
   "source": [
    "#Link to Andrew Ng's video here\n",
    "#Precision is fraction of samples identified as true that really were true\n",
    "    # Precision = number of true positive/(number of true pos + number of false positives) \n",
    "#Recall is fraction of samples that were correctly identified as true, divided by the total number of true samples\n",
    "    # Recall = number of true positive/(number of true positives + number of false negatives)\n",
    "#F1 score is 2* (P*R)/(P+R).  It is used to balance precision and recall\n",
    "\n",
    "\n",
    "#Type I error = false positive\n",
    "#Type II error = false negative\n",
    "#For our purposes, we want to minimize type I errors so no negative tweets sneak in.\n",
    "#We do this by maximizing recall [negative] so that we correctly identify as many negative events as we can\n",
    "\n",
    "cross_valid_accuracy = nltk.classify.accuracy(classifier, cv_set)\n",
    "\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(cv_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "print ('Accuracy:', cross_valid_accuracy)\n",
    "print ('F-measure [negative]:', fmeas(refsets['negative'], testsets['negative']))\n",
    "print ('F-measure [positive]:', fmeas(refsets['positive'], testsets['positive']))\n",
    "print ('Precision [negative]:', prec(refsets['negative'], testsets['negative']))\n",
    "print ('Precision [positive]:', prec(refsets['positive'], testsets['positive']))\n",
    "rec_neg=rec(refsets['negative'], testsets['negative'])\n",
    "rec_pos=rec(refsets['positive'], testsets['positive'])\n",
    "print ('Recall [negative]:', rec_neg)\n",
    "print ('Recall [positive]:', rec_pos)\n",
    "print ('Negative contamination improved by ',100*(1-(1-rec_neg)/(rec_pos)), 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want high negative recall so that we correctly identify and reject negative tweets often.  Ideally, we would have high negative precision as well.  In this case we have low negaive precision, which means we are identifying a lot of tweets as negative when they are not actually negative.  This is okay as long as it is randomly happening. \n",
    "However, if we accidentally identify a positive tweet as negative more often for a specific team, then we are skewing the results for that team. - Throw out any team identifiers in the tweets to solve this problem.\n",
    "\n",
    "Note: If we had high negative precision we could look at which teams are the most hated by looking at the fraction of negative tweets associated with each tam.  However, since we are randomly throwing tweets into the negative category every now and then, it wouldn't work well for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "contains(('team', 'is')) = True           negati : positi =     10.3 : 1.0\n",
      "            contains(at) = True           positi : negati =      9.0 : 1.0\n",
      "          contains(when) = True           negati : positi =      8.3 : 1.0\n",
      "          contains(fuck) = True           negati : positi =      7.7 : 1.0\n",
      "contains(('on', 'team')) = True           negati : positi =      7.0 : 1.0\n",
      "     contains(pokemongo) = True           positi : negati =      6.5 : 1.0\n",
      "          contains(they) = True           negati : positi =      5.0 : 1.0\n",
      "          contains(team) = False          positi : negati =      4.8 : 1.0\n",
      "         contains(trash) = True           negati : positi =      4.3 : 1.0\n",
      "contains(('is', 'team')) = True           negati : positi =      4.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Show the 20 most important features\n",
    "print (classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#collecting top 150 features\n",
    "top_features=classifier.most_informative_features(150)\n",
    "\n",
    "#removing text around each feature\n",
    "top_features=[top_features[row][0].split('contains(')[1][:-1] for row in range(len(top_features))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the top features?  This will overwrite the old save file (yes/no): no\n"
     ]
    }
   ],
   "source": [
    "#Save the top features to text for itterative use\n",
    "do_save = input('Do you want to save the top features?  This will overwrite the old save file (yes/no): ')\n",
    "\n",
    "if do_save == 'yes':\n",
    "    if os.path.isfile('top_features.txt'):\n",
    "        os.remove('top_features.txt')\n",
    "    with open('top_features.txt', 'wb') as f:\n",
    "        pickle.dump(top_features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> After optimizing for CV set, get statistics for test set </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the classifier for later use\n",
    "f = open('PoGo_tweet_classifier.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()\n",
    "\n",
    "#Save document_words as well\n",
    "with open('PoGo_classifier_feats.pickle', 'wb') as f:\n",
    "    pickle.dump(word_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7083333333333334\n",
      "F-measure [negative]: 0.19457735247208932\n",
      "F-measure [positive]: 0.8184106436533621\n",
      "Precision [negative]: 0.11070780399274047\n",
      "Precision [positive]: 0.986990459670425\n",
      "Recall [negative]: 0.8026315789473685\n",
      "Recall [positive]: 0.699017199017199\n",
      "Negative contamination improved by  71.7648691147905 percent\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "print ('Accuracy:', cross_valid_accuracy)\n",
    "print ('F-measure [negative]:', fmeas(refsets['negative'], testsets['negative']))\n",
    "print ('F-measure [positive]:', fmeas(refsets['positive'], testsets['positive']))\n",
    "print ('Precision [negative]:', prec(refsets['negative'], testsets['negative']))\n",
    "print ('Precision [positive]:', prec(refsets['positive'], testsets['positive']))\n",
    "rec_neg=rec(refsets['negative'], testsets['negative'])\n",
    "rec_pos=rec(refsets['positive'], testsets['positive'])\n",
    "print ('Recall [negative]:', rec_neg)\n",
    "print ('Recall [positive]:', rec_pos)\n",
    "print ('Negative contamination improved by ',100*(1-(1-rec_neg)/(rec_pos)), 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate of systematic error from uncut negative tweets:  2.52 percent\n"
     ]
    }
   ],
   "source": [
    "uncut_positives = total_pos*rec_pos\n",
    "uncut_negatives = total_neg*(1-rec_neg)\n",
    "contamination_est = uncut_negatives / uncut_positives\n",
    "print('Estimate of systematic error from uncut negative tweets: ', round(10000*contamination_est)/100, 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original systematic error from uncut negative tweets was:  8.92 percent\n"
     ]
    }
   ],
   "source": [
    "print('Original systematic error from uncut negative tweets was: ', round(10000*(total_neg/total_pos))/100, 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Systematic error improved by a factor of:  3.5416871416871425\n"
     ]
    }
   ],
   "source": [
    "print('Systematic error improved by a factor of: ', (total_neg/total_pos)/contamination_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
